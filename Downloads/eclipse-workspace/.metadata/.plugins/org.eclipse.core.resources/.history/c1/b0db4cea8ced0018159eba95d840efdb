Put your name and netid here

(1) Run the program BenchmarkForAutocomplete and copy/paste the 
results here this for #matches = 20

search	size	#match	binary	brute
	456976	20	0.1759	0.0120
a	17576	20	0.0054	0.0102
b	17576	20	0.0038	0.0056
c	17576	20	0.0040	0.0047
x	17576	20	0.0037	0.0056
y	17576	20	0.0039	0.0060
z	17576	20	0.0036	0.0060
aa	676	20	0.0001	0.0053
az	676	20	0.0001	0.0058
za	676	20	0.0001	0.0055
zz	676	20	0.0001	0.0047

(2) Run the program again for #matches = 10000, paste the results, 
and then make any conclusions about how the # matches 
affects the runtime. 

(3) Copy/paste the code from BruteAutocomplete.topMatches below. 
Explain what the Big-Oh complexity of the entire loop: 
for(Term t : myTerms) {...} 
is in terms of N, the number of elements in myTerms and 
M, the number of terms that match the prefix. 
Assume that every priority-queue operation runs in O(log k) time. 
Explain your answer which should be in terms of N, M, and k.

(4) Explain why the last for loop in BruteAutocomplete.topMatches 
uses a LinkedList (and not an ArrayList) 
AND why the PriorityQueue uses Term.WeightOrder to get 
the top k heaviest matches -- rather than 
using Term.ReverseWeightOrder.


(5) Explain what the runtime of the 
BinarySearchAutocomplete.topMatches code that you 
implemented is by copy/pasting the code below 
and explaining your answer in terms of N, M, and k.


